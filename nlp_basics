{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction**\n   ","metadata":{}},{"cell_type":"markdown","source":"There are two important components to NLP that will be covered here:\n- Regular expressions\n- Tokenization\n\nWith this, two important libraries for the above tasks are:\n1. The re library for regular expression\n2. The NLTK library for tokenization\n\n**What are regular expressions?**\n\nRegular expressions are strings, but with special syntax. Essentially, they give us the ability to *search* for and *identify* text that follows **certain rules or formats**.","metadata":{}},{"cell_type":"markdown","source":"### Some common regular expression methods\n- Here we shall be dealing with the use of regular expressions\n    - Important methods under the re library are:\n        - re.split - To split a string on a regular expression\n        - re.finall() - To find all pattern in a string\n        - re.match() - To match an entire string or substring based on a pattern\n        - re.search() - Search for a pattern in a string\n \n NOTE: \n - The syntax is always:* pattern first, string second*\n - It may also return an iterator, a new string or a match object","metadata":{}},{"cell_type":"markdown","source":"### Common regular expression patterns\n\nIn regular expressions, the + means \"one or more occurrences.\"\n\n\n**Words**\n* \\w ->  Pattern for a single word character\nIt can match any of the following:\n- \"a\" in \"cat\" (one character)\n- \"C\" in \"Cat\" (one character)\n- \"7\" in \"7up\" (one digit)\n- \"_\" in \"hello_world\" (underscore)\n\n\n* \\w+ -> Pattern for words or a continuous sequences of word characters\n\n\\w+ would match:\n- \"cat\" (entire word)\n- \"Cat123\" (letters and numbers)\n- \"hello_world\" (entire string including underscore)\n\n**Digits**\n* \\d ->  Pattern for a numerical character. \nIt only matches one digit at a time. If a number appears as 19, it will treat 1 and 9 as separate numbers\n\n\n* \\d+ -> Pattern for all occurences of whole numbers. \nIt matches entire numbers (multiple digits together)\n\n**Space**\n* \\s ->  Pattern for a single space ' '\n* \\s+ -> Pattern for more than one space in a document/string/text\n\n**Wildcard**\n. -> Is the default wildcard pattern, but it will only match any single character except a newline\n\n.+ -> matches one or more of any character\n\n.* -> matches zero or more of any character (greedy)\n\n**lowercase character[a-z]**\n[a-z] pattern represents a character class that matches any single lowercase letter from 'a' to 'z'. ","metadata":{}},{"cell_type":"code","source":"import re\n\nmy_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n\nre.findall(\"\\w+\", my_string)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-29T00:11:38.866221Z","iopub.execute_input":"2024-10-29T00:11:38.866664Z","iopub.status.idle":"2024-10-29T00:11:38.875134Z","shell.execute_reply.started":"2024-10-29T00:11:38.866624Z","shell.execute_reply":"2024-10-29T00:11:38.873995Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['Let',\n 's',\n 'write',\n 'RegEx',\n 'Won',\n 't',\n 'that',\n 'be',\n 'fun',\n 'I',\n 'sure',\n 'think',\n 'so',\n 'Can',\n 'you',\n 'find',\n '4',\n 'sentences',\n 'Or',\n 'perhaps',\n 'all',\n '19',\n 'words']"},"metadata":{}}]},{"cell_type":"markdown","source":"#### **Using the wildcard pattern**","metadata":{}},{"cell_type":"code","source":"text = \"cat hat rat mat\"\npattern_basic = \"c.t\"         # A pattern of character between W and ?\npattern_more = \"c.+t\"         # A pattern of one or more characters between W and ?\npattern_greedy = \"c.*t\"         # A pattern of characters between W and ?\nre.findall(pattern_basic, text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:19:05.318894Z","iopub.execute_input":"2024-10-29T00:19:05.319313Z","iopub.status.idle":"2024-10-29T00:19:05.327693Z","shell.execute_reply.started":"2024-10-29T00:19:05.319266Z","shell.execute_reply":"2024-10-29T00:19:05.326562Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['cat']"},"metadata":{}}]},{"cell_type":"code","source":"re.findall(pattern_basic, text)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:19:24.075863Z","iopub.execute_input":"2024-10-29T00:19:24.076330Z","iopub.status.idle":"2024-10-29T00:19:24.083862Z","shell.execute_reply.started":"2024-10-29T00:19:24.076246Z","shell.execute_reply":"2024-10-29T00:19:24.082628Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"['cat']"},"metadata":{}}]},{"cell_type":"code","source":"re.findall(pattern_more, text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:19:13.195340Z","iopub.execute_input":"2024-10-29T00:19:13.195795Z","iopub.status.idle":"2024-10-29T00:19:13.203543Z","shell.execute_reply.started":"2024-10-29T00:19:13.195742Z","shell.execute_reply":"2024-10-29T00:19:13.202216Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"['cat hat rat mat']"},"metadata":{}}]},{"cell_type":"markdown","source":"> The wildcard is particularly useful when:\n> \n> You don't care what the character is\n> You want to match patterns with varying content in the middle\n> You need to extract content between known markers","metadata":{}},{"cell_type":"code","source":"# Another example\n\ntext = \"<title>Hello World</title>\"\n\n# Greedy match (.*) - matches everything between first < and last >\ngreedy = re.findall(r'<.*>', text)\nprint(greedy)  \n\n# Lazy match (.*?) - matches minimum needed\nlazy = re.findall(r'<.*?>', text)\nprint(lazy) ","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:59:33.662753Z","iopub.execute_input":"2024-10-29T00:59:33.664424Z","iopub.status.idle":"2024-10-29T00:59:33.675214Z","shell.execute_reply.started":"2024-10-29T00:59:33.664373Z","shell.execute_reply":"2024-10-29T00:59:33.673934Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"['<title>Hello World</title>']\n['<title>', '</title>']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **The square bracket combination or the *character classes* such as the lowercase characters [a-z]**","metadata":{}},{"cell_type":"markdown","source":"Key points about character classes:\n\nThey match exactly one character (unless used with quantifiers like +, *, ?)\nThe range is inclusive (includes both start and end characters)\nMultiple ranges can be combined in a single class\nThe hyphen (-) has special meaning unless it's at the start or end of the class\nSome special characters lose their special meaning inside character classes\n\nWould you like to explore more complex patterns or see specific examples for certain use cases?","metadata":{}},{"cell_type":"markdown","source":"#### **The [a-z] character class**\n\n[a-z] pattern represents a character class that matches any single lowercase letter from 'a' to 'z'. \nNOTE: This is different from \n\\w which will match any character (letters, digits and symbols, etc)","metadata":{}},{"cell_type":"code","source":"# Basic character ranges\ntext = \"The Quick Brown Fox 123 meet the BROWN BEAR that 12 years old\"\n\n# [a-z] matches any lowercase letter\nlowercase = re.findall(r'[a-z]+', text)\nprint(lowercase)  # ['he', 'uick', 'rown', 'ox']\n\n# [A-Z] matches any uppercase letter\nuppercase = re.findall(r'[A-Z]+', text)\nprint(uppercase)  # ['T', 'Q', 'B', 'F']\n\n# [A-Za-z] matches any letter (case insensitive)\nall_letters = re.findall(r'[A-Za-z]+', text)\nprint(all_letters)  # ['The', 'Quick', 'Brown', 'Fox']\n\n# [0-9] matches any digit (similar to \\d)\ndigits = re.findall(r'[0-9]+', text)\nprint(digits)  # ['123']","metadata":{"execution":{"iopub.status.busy":"2024-10-29T01:53:30.053560Z","iopub.execute_input":"2024-10-29T01:53:30.053991Z","iopub.status.idle":"2024-10-29T01:53:30.063948Z","shell.execute_reply.started":"2024-10-29T01:53:30.053953Z","shell.execute_reply":"2024-10-29T01:53:30.062380Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"['he', 'uick', 'rown', 'ox', 'meet', 'the', 'that', 'years', 'old']\n['T', 'Q', 'B', 'F', 'BROWN', 'BEAR']\n['The', 'Quick', 'Brown', 'Fox', 'meet', 'the', 'BROWN', 'BEAR', 'that', 'years', 'old']\n['123', '12']\n","output_type":"stream"}]},{"cell_type":"code","source":"# [A-Za-z0-9] matches letters and numbers\ntext = \"Hello123_World456\"\nalphanumeric = re.findall(r'[A-Za-z0-9]+', text)\nprint(alphanumeric)  # ['Hello123', 'World456']\n\n# Custom ranges\ntext = \"Grade: A+ B- C# D\"\ngrades = re.findall(r'[A-D][#+-]?', text)\nprint(grades)  # ['A+', 'B-', 'C#', 'D']","metadata":{"execution":{"iopub.status.busy":"2024-10-29T01:20:12.383193Z","iopub.execute_input":"2024-10-29T01:20:12.383696Z","iopub.status.idle":"2024-10-29T01:20:12.391237Z","shell.execute_reply.started":"2024-10-29T01:20:12.383650Z","shell.execute_reply":"2024-10-29T01:20:12.389932Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"['Hello123', 'World456']\n['A+', 'B-', 'C#', 'D']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"####  **Negation**","metadata":{}},{"cell_type":"code","source":"# [^a-z] matches anything that's NOT a lowercase letter\ntext = \"Hello a 123!\"\n\nnot_lowercase1 = re.findall(r'[^a-z]+', text)\nprint(not_lowercase1)  # ['H', ' 123!']\n\nnot_lowercase2 = re.findall(r'[A-Z0-9]+', text)\nprint(not_lowercase2)                     # Notice the difference? This does not include the \"!\" character","metadata":{"execution":{"iopub.status.busy":"2024-10-29T01:24:51.122180Z","iopub.execute_input":"2024-10-29T01:24:51.122615Z","iopub.status.idle":"2024-10-29T01:24:51.129852Z","shell.execute_reply.started":"2024-10-29T01:24:51.122576Z","shell.execute_reply":"2024-10-29T01:24:51.128506Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"['H', ' ', ' 123!']\n['H', '123']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Special specific combination of characters**","metadata":{}},{"cell_type":"code","source":"# Include specific characters alongside ranges\ntext = \"user@example.com.my\"\nemail_chars = re.findall(r'[a-zA-Z0-9@.]+', text)\nprint(email_chars)  # ['user@example.com']\n\n# Match specific set of characters\ntext = \"The temperature is 72째F\"\nspecial = re.findall(r'[째CF]+', text)\nprint(special)  # ['째F']","metadata":{"execution":{"iopub.status.busy":"2024-10-29T01:49:38.115661Z","iopub.execute_input":"2024-10-29T01:49:38.116134Z","iopub.status.idle":"2024-10-29T01:49:38.123454Z","shell.execute_reply.started":"2024-10-29T01:49:38.116091Z","shell.execute_reply":"2024-10-29T01:49:38.122300Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"['user@example.com.my']\n['째F']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **User validation cases**","metadata":{}},{"cell_type":"code","source":"# Username validation (allowing letters, numbers, and underscore)\nusername = \"John_Doe123\"\nis_valid = bool(re.match(r'^[A-Za-z][A-Za-z0-9_]*$', username))\nprint(is_valid)  # True\n\n# Extract words with hyphens\ntext = \"This is a well-written post-modern example\"\nhyphenated = re.findall(r'[A-Za-z]+[-][A-Za-z]+', text)\nprint(hyphenated)  # ['well-written', 'post-modern']","metadata":{"execution":{"iopub.status.busy":"2024-10-29T01:51:29.115135Z","iopub.execute_input":"2024-10-29T01:51:29.115565Z","iopub.status.idle":"2024-10-29T01:51:29.124117Z","shell.execute_reply.started":"2024-10-29T01:51:29.115526Z","shell.execute_reply":"2024-10-29T01:51:29.122917Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"True\n['well-written', 'post-modern']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Practicing regular expressions: re.split() and re.findall()**","metadata":{}},{"cell_type":"markdown","source":"Split my_string on each sentence ending. To do this:\n\n    Write a pattern called sentence_endings to match sentence endings (.?!).\n    Use re.split() to split my_string on the pattern and print the result.\n\nFind and print all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall().\n\n    Remember the [a-z] pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.\n\nWrite a pattern called spaces to match one or more spaces (\"\\s+\") and then use re.split() to split my_string on this pattern, keeping all punctuation intact. Print the result.","metadata":{}},{"cell_type":"code","source":"my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n\n# Write a pattern to match sentence endings: sentence_endings\nsentence_endings = r\"[.?!]\"\n\n# Split my_string on sentence endings and print the result\nprint(re.split(sentence_endings, my_string))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T00:41:33.299430Z","iopub.execute_input":"2024-10-28T00:41:33.299964Z","iopub.status.idle":"2024-10-28T00:41:33.309240Z","shell.execute_reply.started":"2024-10-28T00:41:33.299911Z","shell.execute_reply":"2024-10-28T00:41:33.307327Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Find all capitalized words in my_string and print the result\ncapitalized_letters = r\"[A-Z]\"           # finds all occurence of all capital letters\nprint(re.findall(capitalized_letters, my_string))\n\ncapitalized_words = r\"[A-Z]\\w+\"         # finds all occurence of all capital letters\nprint(re.findall(capitalized_words,my_string))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T00:46:43.942065Z","iopub.execute_input":"2024-10-28T00:46:43.942587Z","iopub.status.idle":"2024-10-28T00:46:43.950558Z","shell.execute_reply.started":"2024-10-28T00:46:43.942538Z","shell.execute_reply":"2024-10-28T00:46:43.949222Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['L', 'R', 'E', 'W', 'I', 'C', 'O']\n['Let', 'RegEx', 'Won', 'Can', 'Or']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split my_string on spaces and print the result\nspaces = r\"\\s+\"\nprint(re.split(spaces, my_string))","metadata":{"execution":{"iopub.status.busy":"2024-10-28T00:47:10.547416Z","iopub.execute_input":"2024-10-28T00:47:10.547943Z","iopub.status.idle":"2024-10-28T00:47:10.554985Z","shell.execute_reply.started":"2024-10-28T00:47:10.547890Z","shell.execute_reply":"2024-10-28T00:47:10.553476Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Find all digits in my_string and print the result\ndigits = r\"\\d+\"\nprint(re.findall(digits, my_string))","metadata":{"execution":{"iopub.status.busy":"2024-10-28T00:48:25.088906Z","iopub.execute_input":"2024-10-28T00:48:25.089521Z","iopub.status.idle":"2024-10-28T00:48:25.099240Z","shell.execute_reply.started":"2024-10-28T00:48:25.089465Z","shell.execute_reply":"2024-10-28T00:48:25.097449Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['4', '19']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Introduction to Tokenization**","metadata":{}},{"cell_type":"markdown","source":"Tokenization is the process of converting a string or a document into **smaller chunks** called tokens. This chunk can be just the individual words or characters. It is an important step in the preparation of texts for NLP tasks.\n- It might involve spliting a document into sentences\n- Splitting the sentences into words\n- Splitting the words into characters (e.g., separating all hashtags in a tweet\n\nRules for tokenization can be created using regular expressions. \nAn important library for tokenization is NLTK (Natural language toolkit). Some useful methods from NLTK are:\n- word_tokenize() to split a word into tokens\n- sent_tokenize() to split a document into sentences\n- regexp_tokenize() - use a regular expression pattern to tokenize a document or sentence\n- Tweettokenize() - special method for tokenizing tweets\n","metadata":{}},{"cell_type":"code","source":"### The use of re.search and re.match are very common in most NLTK tasks. But how are they different?\n\nimport re\n\nre.match(\"abc\", \"abcdef\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T01:14:42.710517Z","iopub.execute_input":"2024-10-28T01:14:42.711332Z","iopub.status.idle":"2024-10-28T01:14:42.718894Z","shell.execute_reply.started":"2024-10-28T01:14:42.711279Z","shell.execute_reply":"2024-10-28T01:14:42.717747Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<re.Match object; span=(0, 3), match='abc'>"},"metadata":{}}]},{"cell_type":"code","source":"re.search(\"abc\", \"abcdef\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T01:14:50.964606Z","iopub.execute_input":"2024-10-28T01:14:50.965148Z","iopub.status.idle":"2024-10-28T01:14:50.974252Z","shell.execute_reply.started":"2024-10-28T01:14:50.965097Z","shell.execute_reply":"2024-10-28T01:14:50.972972Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<re.Match object; span=(0, 3), match='abc'>"},"metadata":{}}]},{"cell_type":"code","source":"re.match(\"cd\", \"abcderf\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T01:15:20.559933Z","iopub.execute_input":"2024-10-28T01:15:20.560468Z","iopub.status.idle":"2024-10-28T01:15:20.566620Z","shell.execute_reply.started":"2024-10-28T01:15:20.560416Z","shell.execute_reply":"2024-10-28T01:15:20.565060Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"re.search(\"cd\", \"abcdef\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T01:15:38.142799Z","iopub.execute_input":"2024-10-28T01:15:38.143294Z","iopub.status.idle":"2024-10-28T01:15:38.151597Z","shell.execute_reply.started":"2024-10-28T01:15:38.143248Z","shell.execute_reply":"2024-10-28T01:15:38.150207Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<re.Match object; span=(2, 4), match='cd'>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### **Word tokenization with NLTK**","metadata":{}},{"cell_type":"markdown","source":"Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n\nYour job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.\n\n\n    Import the sent_tokenize and word_tokenize functions from nltk.tokenize.\n    Tokenize all the sentences in scene_one using the sent_tokenize() function.\n    Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.\n    Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().\n    Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!\n","metadata":{}},{"cell_type":"code","source":"scene_one = \"\"\"SCENE 1: [wind] [clop clop clop] \nKING ARTHUR: Whoa there!  [clop clop clop] \nSOLDIER #1: Halt!  Who goes there?\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\nSOLDIER #1: Pull the other one!\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\nSOLDIER #1: What?  Ridden on a horse?\nARTHUR: Yes!\nSOLDIER #1: You're using coconuts!\nARTHUR: What?\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\nSOLDIER #1: Where'd you get the coconuts?\nARTHUR: We found them.\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\nARTHUR: What do you mean?\nSOLDIER #1: Well, this is a temperate zone.\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\nSOLDIER #1: Are you suggesting coconuts migrate?\nARTHUR: Not at all.  They could be carried.\nSOLDIER #1: What?  A swallow carrying a coconut?\nARTHUR: It could grip it by the husk!\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\nARTHUR: Please!\nSOLDIER #1: Am I right?\nARTHUR: I'm not interested!\nSOLDIER #2: It could be carried by an African swallow!\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\nSOLDIER #2: Oh, yeah, I agree with that.\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\nSOLDIER #2: Oh, yeah...\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\nSOLDIER #1: No, they'd have to have it on a line.\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\nSOLDIER #1: What, held under the dorsal guiding feathers?\nSOLDIER #2: Well, why not?\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-28T01:31:08.362086Z","iopub.execute_input":"2024-10-28T01:31:08.362617Z","iopub.status.idle":"2024-10-28T01:31:08.371991Z","shell.execute_reply.started":"2024-10-28T01:31:08.362564Z","shell.execute_reply":"2024-10-28T01:31:08.370327Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Import necessary modules\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n# Split scene_one into sentences: sentences\nsentences = sent_tokenize(scene_one)\n\n# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\ntokenized_sent = word_tokenize(sentences[3])\n\n# Make a set of unique tokens in the entire scene: unique_tokens\nunique_tokens = set(word_tokenize(scene_one))\n\n# Print the unique tokens result\nprint(unique_tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T01:31:12.098616Z","iopub.execute_input":"2024-10-28T01:31:12.099199Z","iopub.status.idle":"2024-10-28T01:31:13.847851Z","shell.execute_reply.started":"2024-10-28T01:31:12.099144Z","shell.execute_reply":"2024-10-28T01:31:13.846387Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{'ridden', 'bangin', 'The', 'winter', 'may', 'plover', 'course', 'sun', 'with', 'You', 'is', 'Who', 'trusty', 'at', \"'em\", 'are', 'question', 'Please', 'held', 'then', 'the', \"'\", 'on', 'where', 'forty-three', 'that', 'from', 'King', 'there', 'of', 'martin', 'agree', 'mean', 'simple', 'air-speed', 'two', 'In', 'European', ':', 'together', 'back', 'covered', 'master', 'I', 'times', 'Oh', 'under', 'zone', 'wings', 'grips', \"n't\", 'tropical', 'who', 'husk', \"'m\", 'We', 'five', \"'ve\", 'anyway', '#', 'yeah', 'why', 'all', 'Pendragon', 'carrying', 'south', 'grip', 'will', 'here', 'it', 'an', 'Saxons', 'court', 'coconuts', 'Will', 'must', '--', 'son', 'That', 'strangers', 'line', 'Well', 'bird', '...', 'not', 'defeator', 'Patsy', 'It', 'found', 'servant', 'my', 'Where', \"'d\", 'beat', 'length', 'fly', 'creeper', '[', 'since', 'climes', 'carry', 'Am', 'or', 'every', 'dorsal', 'they', 'Mercea', 'KING', 'Uther', 'in', 'snows', \"'re\", 'SCENE', 'maintain', 'if', 'knights', 'Not', 'Found', ']', 'he', 'second', 'your', 'use', 'speak', 'goes', 'Yes', 'go', 'velocity', 'needs', 'suggesting', 'empty', 'warmer', 'these', 'them', 'matter', 'castle', 'weight', 'Arthur', 'ARTHUR', 'Camelot', 'join', 'strand', 'What', 'Listen', 'and', 'search', 'a', 'sovereign', 'Whoa', '.', 'other', 'wants', 'point', 'temperate', 'this', 'maybe', 'wind', 'breadth', 'ounce', 'interested', 'do', 'African', 'Ridden', 'right', 'using', 'halves', '2', 'have', 'migrate', '1', 'ask', 'but', 'you', 'am', \"'s\", 'Pull', 'pound', 'A', 'swallows', 'got', 'Halt', 'lord', 'me', 'coconut', 'yet', 'Are', 'feathers', 'Court', 'our', 'just', 'So', 'bring', 'No', 'England', 'kingdom', 'to', 'Britons', 'its', 'horse', 'order', 'be', 'seek', ',', 'clop', '!', 'house', 'by', 'tell', 'SOLDIER', 'But', 'carried', 'get', 'non-migratory', 'minute', 'Supposing', 'one', 'through', 'Wait', 'guiding', 'ratios', 'swallow', 'land', 'could', 'They', '?', 'does'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **More regex with re.search()**","metadata":{}},{"cell_type":"markdown","source":"In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n\nYou have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.\n\n> \n    Use re.search() to search for the first occurrence of the word \"coconuts\" in scene_one. Store the result in match.\n    Print the start and end indexes of match using its .start() and .end() methods, respectively.\n","metadata":{}},{"cell_type":"code","source":"match = re.search(\"coconuts\", scene_one)\nprint(match.start())\nprint(match.end())","metadata":{"execution":{"iopub.status.busy":"2024-10-28T01:50:36.337672Z","iopub.execute_input":"2024-10-28T01:50:36.339108Z","iopub.status.idle":"2024-10-28T01:50:36.346283Z","shell.execute_reply.started":"2024-10-28T01:50:36.339045Z","shell.execute_reply":"2024-10-28T01:50:36.344958Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"580\n588\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}