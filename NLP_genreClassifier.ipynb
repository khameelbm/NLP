{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9838951,"sourceType":"datasetVersion","datasetId":6035568}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Building a fake news classifier**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"1. Here, we shall build or learn vectors from the movie plot and genre dataset\n2. In short, there is a dataset full of movie plots and what genre the movie is (Action or Sci-Fi)\n3. We wish to create bag of words vectors for this movie plots to see if we can predict the genre based on the words used in the plot summary\n4. To do so, we shall employ the following methods from scikit-learn:\n5. * Load the data\n   * Define the label, y\n   * Split the data into train and test\n   * Create the Countervectorizer object which turns the text into **bags of words**, this is similar to a Gensim corpus. NOTE: as a pre-processing step, **ensure english stop words are removed during the formation of the bad of words****.\n   * Each token will now act as feature for the classifier\n   * Use the .fit_transform() method on the training data (bag_of_word object) to create the bad of words vectors.\n   * Generally, fit_transform() will create the bag of words dictionary and vectors for each documents using the training data\n   * Use the transformation for the training on the test data as well.\n  \n     **Feature Extraction**: This chapter also dealt with feature extraction. Simply put, feature extraction dealt with how to transform text into numerical data using techniques like bag of words models or TF-IDF, essential for processing by machine learning algorithms.\n\n     **Scikit-learn for Model Training**: We will also see how use Scikit-learn, a powerful library for machine learning, to train a model on textual data.\n     \n    **NOTE**: Possible Features for Text Classification: Discussion on what could be considered as features in a text classification problem, such as the number of words, named entities, or the language of the document.\n\n   **DATA**: Recall that the data we are using is the IMDB plot summaries, with labels indicating if the movie is sci-fi or action.\n     ","metadata":{}},{"cell_type":"markdown","source":"### **CountVectorizer for text classification**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/fake-or-real-news/fake_or_real_news.csv\")         #load data\nprint(df.head(5))\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T06:40:37.436951Z","iopub.execute_input":"2024-11-09T06:40:37.437417Z","iopub.status.idle":"2024-11-09T06:40:39.649253Z","shell.execute_reply.started":"2024-11-09T06:40:37.437350Z","shell.execute_reply":"2024-11-09T06:40:39.647772Z"}},"outputs":[{"name":"stdout","text":"   Unnamed: 0                                              title  \\\n0        8476                       You Can Smell Hillary’s Fear   \n1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n2        3608        Kerry to go to Paris in gesture of sympathy   \n3       10142  Bernie supporters on Twitter erupt in anger ag...   \n4         875   The Battle of New York: Why This Primary Matters   \n\n                                                text label  \n0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n4  It's primary day in New York and front-runners...  REAL  \nIndex(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"> Import CountVectorizer from sklearn.feature_extraction.text and train_test_split from sklearn.model_selection.\n    Create a Series y to use for the labels by assigning the .label attribute of df to y.\n    Using df[\"text\"] (features) and y (labels), create training and test sets using train_test_split(). Use a test_size of 0.33 and a random_state of 53.\n> \n   \n>  Create a CountVectorizer object called count_vectorizer. Ensure you specify the keyword argument stop_words=\"english\" so that stop words are removed.\n> \n\n> Fit and transform the training data X_train using the .fit_transform() method of your CountVectorizer object. Do the same with the test data X_test, except using the .transform() method.\n>\n> Print the first 10 features of the count_vectorizer using its .get_feature_names() method.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words = \"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names_out()[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:26:34.158319Z","iopub.execute_input":"2024-11-09T07:26:34.158791Z","iopub.status.idle":"2024-11-09T07:26:40.387634Z","shell.execute_reply.started":"2024-11-09T07:26:34.158752Z","shell.execute_reply":"2024-11-09T07:26:40.386168Z"}},"outputs":[{"name":"stdout","text":"['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n '000km']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### **TfidfVectorizer for text classification**","metadata":{}},{"cell_type":"markdown","source":"> Similar to the sparse CountVectorizer created in the previous exercise, we'll work on creating tf-idf vectors for our documents here.\n>\n> We'll set up a TfidfVectorizer and investigate some of its features.","metadata":{}},{"cell_type":"markdown","source":"> Import TfidfVectorizer from sklearn.feature_extraction.text.\n>\n>  Create a TfidfVectorizer object called tfidf_vectorizer. When doing so, specify the keyword arguments stop_words=\"english\" and max_df=0.7.\n>\n> Fit and transform the training data.\n>  Transform the test data.\n> Print the first 10 features of tfidf_vectorizer.\n>\n> Print the first 5 vectors of the tfidf training data using slicing on the .A (or array) attribute of tfidf_train.\n","metadata":{}},{"cell_type":"code","source":"# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import csr_matrix\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df = 0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Print the first 10 features\nprint(tfidf_vectorizer.get_feature_names_out()[:10])\n\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.toarray()[:15])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:26:45.398088Z","iopub.execute_input":"2024-11-09T07:26:45.398512Z","iopub.status.idle":"2024-11-09T07:26:53.654707Z","shell.execute_reply.started":"2024-11-09T07:26:45.398472Z","shell.execute_reply":"2024-11-09T07:26:53.653553Z"}},"outputs":[{"name":"stdout","text":"['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n '000km']\n[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.05687994 0.         0.         ... 0.         0.         0.        ]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(count_train.toarray()[:10])\nprint(count_train.get_feature_names_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T06:48:39.451415Z","iopub.execute_input":"2024-11-09T06:48:39.452274Z","iopub.status.idle":"2024-11-09T06:48:40.528300Z","shell.execute_reply.started":"2024-11-09T06:48:39.452218Z","shell.execute_reply":"2024-11-09T06:48:40.526475Z"}},"outputs":[{"name":"stdout","text":"[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 1 0 ... 0 0 0]\n [0 1 0 ... 0 0 0]]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(count_train\u001b[38;5;241m.\u001b[39mtoarray()[:\u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcount_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m)\n","\u001b[0;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'get_feature_names_out'"],"ename":"AttributeError","evalue":"'csr_matrix' object has no attribute 'get_feature_names_out'","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"We shall now compare ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### **Exercise - Inspecting the vectors**","metadata":{}},{"cell_type":"code","source":"# Create the CountVectorizer DataFrame: count_df\ncount_df = pd.DataFrame(count_train.toarray(), \ncolumns=count_vectorizer.get_feature_names_out())\n\n# Create the TfidfVectorizer DataFrame: tfidf_df\ntfidf_df = pd.DataFrame(tfidf_train.toarray(), columns = tfidf_vectorizer.get_feature_names_out())\n\n# Print the head of count_df\nprint(count_df.head())\n\n# Print the head of tfidf_df\nprint(tfidf_df.head())\n\n# Calculate the difference in columns: difference\ndifference = set(tfidf_df.columns) - set(count_df.columns)\nprint(difference)\n\n# Check whether the DataFrames are equal\nprint(count_df.equals(tfidf_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:34:01.372824Z","iopub.execute_input":"2024-11-09T07:34:01.373321Z","iopub.status.idle":"2024-11-09T07:34:04.765510Z","shell.execute_reply.started":"2024-11-09T07:34:01.373275Z","shell.execute_reply":"2024-11-09T07:34:04.764198Z"}},"outputs":[{"name":"stdout","text":"   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n0   0    0     0         0       0      0     0       0      0      0  ...   \n1   0    0     0         0       0      0     0       0      0      0  ...   \n2   0    0     0         0       0      0     0       0      0      0  ...   \n3   0    0     0         0       0      0     0       0      0      0  ...   \n4   0    0     0         0       0      0     0       0      0      0  ...   \n\n   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n0    0     0   0   0   0        0   0    0        0      0  \n1    0     0   0   0   0        0   0    0        0      0  \n2    0     0   0   0   0        0   0    0        0      0  \n3    0     0   0   0   0        0   0    0        0      0  \n4    0     0   0   0   0        0   0    0        0      0  \n\n[5 rows x 56922 columns]\n    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n\n   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n\n[5 rows x 56922 columns]\nset()\nFalse\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### **Building a Naive Baye's classifier**","metadata":{}},{"cell_type":"markdown","source":"Here we shall be building a supervised machine learning model for \"fake news\" detection, focusing on the use of the Naive Bayes model for text classification. \n\n> The Naive Bayes algorithm, based on probability, helps classify text by determining how likely a piece of data leads to a particular outcome. This method is particularly effective for natural language processing (NLP) classification problems due to its simplicity and efficiency, despite the availability of more complex models and algorithms.","metadata":{}},{"cell_type":"markdown","source":"> Recall that the data we are using is the IMDB plot summaries, with labels indicating if the movie is sci-fi or action.\n>\n> NOTE: The Naive Bayes in scikit-learn expects integer input. So, it does work well with the output from the Count Vectorizer\n>\n> NOTE: Below, while evaluating the confusion matrix, we specify labels. If we don't specify labels, scikit-learn will use python ordering.\n\n**Evaluating Model Performance**: The accuracy of the model will be assessed using the accuracy_score function from scikit-learn's metrics module. Additionally, we shall interpret the confusion matrix to understand the model's performance in classifying 'FAKE' and 'REAL' news articles.","metadata":{}},{"cell_type":"markdown","source":"#### **Training and testing the \"fake news\" model with CountVectorizer**","metadata":{}},{"cell_type":"code","source":"# Import the necessary modules\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(count_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(count_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE', 'REAL'])\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T08:21:36.127037Z","iopub.execute_input":"2024-11-09T08:21:36.127504Z","iopub.status.idle":"2024-11-09T08:21:36.209111Z","shell.execute_reply.started":"2024-11-09T08:21:36.127462Z","shell.execute_reply":"2024-11-09T08:21:36.207756Z"}},"outputs":[{"name":"stdout","text":"0.893352462936394\n[[ 865  143]\n [  80 1003]]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### **Training and testing the \"fake news\" model with  TfidfVectorizer**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred,labels=['FAKE', 'REAL'])\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T08:37:17.694069Z","iopub.execute_input":"2024-11-09T08:37:17.694624Z","iopub.status.idle":"2024-11-09T08:37:17.768421Z","shell.execute_reply.started":"2024-11-09T08:37:17.694577Z","shell.execute_reply":"2024-11-09T08:37:17.767008Z"}},"outputs":[{"name":"stdout","text":"0.8565279770444764\n[[ 739  269]\n [  31 1052]]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### **Improving your model**","metadata":{}},{"cell_type":"markdown","source":"**Here, we shall investigate the effect of one of the hyperparameter of the Naive Baye's method.**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/fake-or-real-news/fake_or_real_news.csv\")         #load data\n#print(df.head(5))\n\n# Create a series to store the labels: y\ny = df[\"label\"]                # The label column\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T09:51:56.596322Z","iopub.execute_input":"2024-11-10T09:51:56.596856Z","iopub.status.idle":"2024-11-10T09:51:57.042484Z","shell.execute_reply.started":"2024-11-10T09:51:56.596713Z","shell.execute_reply":"2024-11-10T09:51:57.041036Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#from scipy.sparse import csr_matrix\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df = 0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Print the first 10 features\nprint(tfidf_vectorizer.get_feature_names_out()[:10])\n\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.toarray()[:15])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T09:52:00.608493Z","iopub.execute_input":"2024-11-10T09:52:00.609343Z","iopub.status.idle":"2024-11-10T09:52:08.390037Z","shell.execute_reply.started":"2024-11-10T09:52:00.609292Z","shell.execute_reply":"2024-11-10T09:52:08.388402Z"}},"outputs":[{"name":"stdout","text":"['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n '000km']\n[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.05687994 0.         0.         ... 0.         0.         0.        ]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n# Create the list of alphas: alphas\n\nalphas = np.arange(0, 1, 0.1)\n\n# Define train_and_predict()\ndef train_and_predict(alpha):\n    # Instantiate the classifier: nb_classifier\n    nb_classifier = MultinomialNB(alpha = alpha)\n    \n    # Fit to the training data\n    nb_classifier.fit(tfidf_train, y_train)\n    \n    # Predict the labels: pred\n    pred = nb_classifier.predict(tfidf_test)\n    \n    # Compute accuracy: score\n    score = accuracy_score(y_test, pred)\n    return score\n\n# Iterate over the alphas and print the corresponding score\nfor alpha in alphas:\n    print('Alpha: ', alpha)\n    print('Score: ', train_and_predict(alpha))\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:09:18.235870Z","iopub.execute_input":"2024-11-10T10:09:18.237138Z","iopub.status.idle":"2024-11-10T10:09:18.575511Z","shell.execute_reply.started":"2024-11-10T10:09:18.237089Z","shell.execute_reply":"2024-11-10T10:09:18.574323Z"}},"outputs":[{"name":"stdout","text":"Alpha:  0.0\nScore:  0.8813964610234337\n\nAlpha:  0.1\nScore:  0.8976566236250598\n\nAlpha:  0.2\nScore:  0.8938307030129125\n\nAlpha:  0.30000000000000004\nScore:  0.8900047824007652\n\nAlpha:  0.4\nScore:  0.8857006217120995\n\nAlpha:  0.5\nScore:  0.8842659014825442\n\nAlpha:  0.6000000000000001\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:629: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:635: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Score:  0.874701099952176\n\nAlpha:  0.7000000000000001\nScore:  0.8703969392635102\n\nAlpha:  0.8\nScore:  0.8660927785748446\n\nAlpha:  0.9\nScore:  0.8589191774270684\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"**We can now map the important vector weights back to actual words using some simple inspection techniques.** \n\nPrint the top 20 weighted features for the first label of class_labels and print the bottom 20 weighted features for the second label of class_labels. This has been done for you.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB(alpha = 0.5)\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = confusion_matrix(y_test, pred,labels=['FAKE', 'REAL'])\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:11:47.668001Z","iopub.execute_input":"2024-11-10T10:11:47.668418Z","iopub.status.idle":"2024-11-10T10:11:47.728206Z","shell.execute_reply.started":"2024-11-10T10:11:47.668378Z","shell.execute_reply":"2024-11-10T10:11:47.726786Z"}},"outputs":[{"name":"stdout","text":"0.8842659014825442\n[[ 808  200]\n [  42 1041]]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Get the class labels: class_labels\nclass_labels = nb_classifier.classes_\n\n# Extract the features: feature_names\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(nb_classifier.feature_log_prob_[0],feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint(class_labels[0], feat_with_weights[:20])\n\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint(class_labels[1], feat_with_weights[-20:])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:15:25.995988Z","iopub.execute_input":"2024-11-10T10:15:25.996481Z","iopub.status.idle":"2024-11-10T10:15:26.163560Z","shell.execute_reply.started":"2024-11-10T10:15:25.996438Z","shell.execute_reply":"2024-11-10T10:15:26.162373Z"}},"outputs":[{"name":"stdout","text":"FAKE [(-11.529191688198756, '00000031'), (-11.529191688198756, '00006'), (-11.529191688198756, '000ft'), (-11.529191688198756, '001'), (-11.529191688198756, '002'), (-11.529191688198756, '003'), (-11.529191688198756, '006'), (-11.529191688198756, '008'), (-11.529191688198756, '010'), (-11.529191688198756, '013'), (-11.529191688198756, '025'), (-11.529191688198756, '027'), (-11.529191688198756, '035'), (-11.529191688198756, '037'), (-11.529191688198756, '040'), (-11.529191688198756, '044'), (-11.529191688198756, '048'), (-11.529191688198756, '066'), (-11.529191688198756, '068'), (-11.529191688198756, '075')]\nREAL [(-7.611760822717352, 'president'), (-7.596887245110436, 'american'), (-7.587846888505773, 'media'), (-7.582180995208038, 'donald'), (-7.581029757693777, 'october'), (-7.563695582092016, 'government'), (-7.5026656393893925, 'like'), (-7.495597360162555, 'war'), (-7.4884547344946775, 'new'), (-7.4814927738232395, 'world'), (-7.4572092141138455, 'just'), (-7.328307731326699, 'said'), (-7.319841712431456, 'russia'), (-7.266953087090176, 'fbi'), (-7.172860785558294, '2016'), (-7.122289656209633, 'election'), (-7.108890832123804, 'people'), (-6.800031854551334, 'hillary'), (-6.484936848133225, 'clinton'), (-6.428743953169665, 'trump')]\n","output_type":"stream"}],"execution_count":21}]}