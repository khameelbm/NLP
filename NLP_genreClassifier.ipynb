{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9838951,"sourceType":"datasetVersion","datasetId":6035568}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Building a fake news classifier**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"1. Here, we shall build or learn vectors from the movie plot and genre dataset\n2. In short, there is a dataset full of movie plots and what genre the movie is (Action or Sci-Fi)\n3. We wish to create bag of words vectors for this movie plots to see if we can predict the genre based on the words used in the plot summary\n4. To do so, we shall employ the following methods from scikit-learn:\n5. * Load the data\n   * Define the label, y\n   * Split the data into train and test\n   * Create the Countervectorizer object which turns the text into **bags of words**, this is similar to a Gensim corpus. NOTE: as a pre-processing step, **ensure english stop words are removed during the formation of the bad of words****.\n   * Each token will now act as feature for the classifier\n   * Use the .fit_transform() method on the training data (bag_of_word object) to create the bad of words vectors.\n   * Generally, fit_transform() will create the bag of words dictionary and vectors for each documents using the training data\n   * Use the transformation for the training on the test data as well.\n     ","metadata":{}},{"cell_type":"markdown","source":"### **CountVectorizer for text classification**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/fake-or-real-news/fake_or_real_news.csv\")         #load data\nprint(df.head(5))\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T01:50:07.519310Z","iopub.execute_input":"2024-11-08T01:50:07.520385Z","iopub.status.idle":"2024-11-08T01:50:07.959689Z","shell.execute_reply.started":"2024-11-08T01:50:07.520331Z","shell.execute_reply":"2024-11-08T01:50:07.958423Z"}},"outputs":[{"name":"stdout","text":"   Unnamed: 0                                              title  \\\n0        8476                       You Can Smell Hillary’s Fear   \n1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n2        3608        Kerry to go to Paris in gesture of sympathy   \n3       10142  Bernie supporters on Twitter erupt in anger ag...   \n4         875   The Battle of New York: Why This Primary Matters   \n\n                                                text label  \n0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n4  It's primary day in New York and front-runners...  REAL  \nIndex(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"> Import CountVectorizer from sklearn.feature_extraction.text and train_test_split from sklearn.model_selection.\n    Create a Series y to use for the labels by assigning the .label attribute of df to y.\n    Using df[\"text\"] (features) and y (labels), create training and test sets using train_test_split(). Use a test_size of 0.33 and a random_state of 53.\n> \n   \n>  Create a CountVectorizer object called count_vectorizer. Ensure you specify the keyword argument stop_words=\"english\" so that stop words are removed.\n> \n\n> Fit and transform the training data X_train using the .fit_transform() method of your CountVectorizer object. Do the same with the test data X_test, except using the .transform() method.\n>\n> Print the first 10 features of the count_vectorizer using its .get_feature_names() method.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words = \"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names_out()[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T01:53:56.202276Z","iopub.execute_input":"2024-11-08T01:53:56.202782Z","iopub.status.idle":"2024-11-08T01:54:00.791121Z","shell.execute_reply.started":"2024-11-08T01:53:56.202737Z","shell.execute_reply":"2024-11-08T01:54:00.789908Z"}},"outputs":[{"name":"stdout","text":"['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n '000km']\n","output_type":"stream"}],"execution_count":11}]}