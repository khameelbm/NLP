{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9838951,"sourceType":"datasetVersion","datasetId":6035568}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Building a fake news classifier**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"1. Here, we shall build or learn vectors from the movie plot and genre dataset\n2. In short, there is a dataset full of movie plots and what genre the movie is (Action or Sci-Fi)\n3. We wish to create bag of words vectors for this movie plots to see if we can predict the genre based on the words used in the plot summary\n4. To do so, we shall employ the following methods from scikit-learn:\n5. * Load the data\n   * Define the label, y\n   * Split the data into train and test\n   * Create the Countervectorizer object which turns the text into **bags of words**, this is similar to a Gensim corpus. NOTE: as a pre-processing step, **ensure english stop words are removed during the formation of the bad of words****.\n   * Each token will now act as feature for the classifier\n   * Use the .fit_transform() method on the training data (bag_of_word object) to create the bad of words vectors.\n   * Generally, fit_transform() will create the bag of words dictionary and vectors for each documents using the training data\n   * Use the transformation for the training on the test data as well.\n  \n     **Feature Extraction**: This chapter also dealt with feature extraction. Simply put, feature extraction dealt with how to transform text into numerical data using techniques like bag of words models or TF-IDF, essential for processing by machine learning algorithms.\n\n     **Scikit-learn for Model Training**: We will also see how use Scikit-learn, a powerful library for machine learning, to train a model on textual data.\n     \n    **NOTE**: Possible Features for Text Classification: Discussion on what could be considered as features in a text classification problem, such as the number of words, named entities, or the language of the document.\n\n   **DATA**: Recall that the data we are using is the IMDB plot summaries, with labels indicating if the movie is sci-fi or action.\n     ","metadata":{}},{"cell_type":"markdown","source":"### **CountVectorizer for text classification**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/fake-or-real-news/fake_or_real_news.csv\")         #load data\nprint(df.head(5))\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T06:40:37.436951Z","iopub.execute_input":"2024-11-09T06:40:37.437417Z","iopub.status.idle":"2024-11-09T06:40:39.649253Z","shell.execute_reply.started":"2024-11-09T06:40:37.437350Z","shell.execute_reply":"2024-11-09T06:40:39.647772Z"}},"outputs":[{"name":"stdout","text":"   Unnamed: 0                                              title  \\\n0        8476                       You Can Smell Hillary’s Fear   \n1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n2        3608        Kerry to go to Paris in gesture of sympathy   \n3       10142  Bernie supporters on Twitter erupt in anger ag...   \n4         875   The Battle of New York: Why This Primary Matters   \n\n                                                text label  \n0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n4  It's primary day in New York and front-runners...  REAL  \nIndex(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"> Import CountVectorizer from sklearn.feature_extraction.text and train_test_split from sklearn.model_selection.\n    Create a Series y to use for the labels by assigning the .label attribute of df to y.\n    Using df[\"text\"] (features) and y (labels), create training and test sets using train_test_split(). Use a test_size of 0.33 and a random_state of 53.\n> \n   \n>  Create a CountVectorizer object called count_vectorizer. Ensure you specify the keyword argument stop_words=\"english\" so that stop words are removed.\n> \n\n> Fit and transform the training data X_train using the .fit_transform() method of your CountVectorizer object. Do the same with the test data X_test, except using the .transform() method.\n>\n> Print the first 10 features of the count_vectorizer using its .get_feature_names() method.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words = \"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names_out()[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:26:34.158319Z","iopub.execute_input":"2024-11-09T07:26:34.158791Z","iopub.status.idle":"2024-11-09T07:26:40.387634Z","shell.execute_reply.started":"2024-11-09T07:26:34.158752Z","shell.execute_reply":"2024-11-09T07:26:40.386168Z"}},"outputs":[{"name":"stdout","text":"['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n '000km']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### **TfidfVectorizer for text classification**","metadata":{}},{"cell_type":"markdown","source":"> Similar to the sparse CountVectorizer created in the previous exercise, we'll work on creating tf-idf vectors for our documents here.\n>\n> We'll set up a TfidfVectorizer and investigate some of its features.","metadata":{}},{"cell_type":"markdown","source":"> Import TfidfVectorizer from sklearn.feature_extraction.text.\n>\n>  Create a TfidfVectorizer object called tfidf_vectorizer. When doing so, specify the keyword arguments stop_words=\"english\" and max_df=0.7.\n>\n> Fit and transform the training data.\n>  Transform the test data.\n> Print the first 10 features of tfidf_vectorizer.\n>\n> Print the first 5 vectors of the tfidf training data using slicing on the .A (or array) attribute of tfidf_train.\n","metadata":{}},{"cell_type":"code","source":"# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import csr_matrix\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df = 0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Print the first 10 features\nprint(tfidf_vectorizer.get_feature_names_out()[:10])\n\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.toarray()[:15])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:26:45.398088Z","iopub.execute_input":"2024-11-09T07:26:45.398512Z","iopub.status.idle":"2024-11-09T07:26:53.654707Z","shell.execute_reply.started":"2024-11-09T07:26:45.398472Z","shell.execute_reply":"2024-11-09T07:26:53.653553Z"}},"outputs":[{"name":"stdout","text":"['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n '000km']\n[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.05687994 0.         0.         ... 0.         0.         0.        ]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(count_train.toarray()[:10])\nprint(count_train.get_feature_names_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T06:48:39.451415Z","iopub.execute_input":"2024-11-09T06:48:39.452274Z","iopub.status.idle":"2024-11-09T06:48:40.528300Z","shell.execute_reply.started":"2024-11-09T06:48:39.452218Z","shell.execute_reply":"2024-11-09T06:48:40.526475Z"}},"outputs":[{"name":"stdout","text":"[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 1 0 ... 0 0 0]\n [0 1 0 ... 0 0 0]]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(count_train\u001b[38;5;241m.\u001b[39mtoarray()[:\u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcount_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m)\n","\u001b[0;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'get_feature_names_out'"],"ename":"AttributeError","evalue":"'csr_matrix' object has no attribute 'get_feature_names_out'","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"We shall now compare ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### **Exercise - Inspecting the vectors**","metadata":{}},{"cell_type":"code","source":"# Create the CountVectorizer DataFrame: count_df\ncount_df = pd.DataFrame(count_train.toarray(), \ncolumns=count_vectorizer.get_feature_names_out())\n\n# Create the TfidfVectorizer DataFrame: tfidf_df\ntfidf_df = pd.DataFrame(tfidf_train.toarray(), columns = tfidf_vectorizer.get_feature_names_out())\n\n# Print the head of count_df\nprint(count_df.head())\n\n# Print the head of tfidf_df\nprint(tfidf_df.head())\n\n# Calculate the difference in columns: difference\ndifference = set(tfidf_df.columns) - set(count_df.columns)\nprint(difference)\n\n# Check whether the DataFrames are equal\nprint(count_df.equals(tfidf_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:34:01.372824Z","iopub.execute_input":"2024-11-09T07:34:01.373321Z","iopub.status.idle":"2024-11-09T07:34:04.765510Z","shell.execute_reply.started":"2024-11-09T07:34:01.373275Z","shell.execute_reply":"2024-11-09T07:34:04.764198Z"}},"outputs":[{"name":"stdout","text":"   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n0   0    0     0         0       0      0     0       0      0      0  ...   \n1   0    0     0         0       0      0     0       0      0      0  ...   \n2   0    0     0         0       0      0     0       0      0      0  ...   \n3   0    0     0         0       0      0     0       0      0      0  ...   \n4   0    0     0         0       0      0     0       0      0      0  ...   \n\n   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n0    0     0   0   0   0        0   0    0        0      0  \n1    0     0   0   0   0        0   0    0        0      0  \n2    0     0   0   0   0        0   0    0        0      0  \n3    0     0   0   0   0        0   0    0        0      0  \n4    0     0   0   0   0        0   0    0        0      0  \n\n[5 rows x 56922 columns]\n    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n\n   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n\n[5 rows x 56922 columns]\nset()\nFalse\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### **Building a Naive Baye's classifier**","metadata":{}},{"cell_type":"markdown","source":"> Recall that the data we are using is the IMDB plot summaries, with labels indicating if the movie is sci-fi or action.\n>\n> NOTE: The Naive Bayes in scikit-learn expects integer input. So, it does work well with the output from the Count Vectorizer\n>\n> NOTE: Below, while evaluating the confusion matrix, we specify labels. If we don't specify labels, scikit-learn will use python ordering.","metadata":{}},{"cell_type":"markdown","source":"#### **Training and testing the \"fake news\" model with CountVectorizer**","metadata":{}},{"cell_type":"code","source":"# Import the necessary modules\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(count_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(count_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE', 'REAL'])\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T08:21:36.127037Z","iopub.execute_input":"2024-11-09T08:21:36.127504Z","iopub.status.idle":"2024-11-09T08:21:36.209111Z","shell.execute_reply.started":"2024-11-09T08:21:36.127462Z","shell.execute_reply":"2024-11-09T08:21:36.207756Z"}},"outputs":[{"name":"stdout","text":"0.893352462936394\n[[ 865  143]\n [  80 1003]]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### **Training and testing the \"fake news\" model with  TfidfVectorizer**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred,labels=['FAKE', 'REAL'])\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T08:37:17.694069Z","iopub.execute_input":"2024-11-09T08:37:17.694624Z","iopub.status.idle":"2024-11-09T08:37:17.768421Z","shell.execute_reply.started":"2024-11-09T08:37:17.694577Z","shell.execute_reply":"2024-11-09T08:37:17.767008Z"}},"outputs":[{"name":"stdout","text":"0.8565279770444764\n[[ 739  269]\n [  31 1052]]\n","output_type":"stream"}],"execution_count":17}]}